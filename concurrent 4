import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor
import os

def fetch_url(url):
    """Fetches the content of a URL."""
    try:
        response = requests.get(url, timeout=10)
        return response.text
    except requests.RequestException as e:
        return f"Error: {e}"

def process_chunk(chunk, executor):
    """Processes a chunk of the DataFrame, adding the fetched URL content."""
    chunk['response'] = list(executor.map(fetch_url, chunk['urls']))
    return chunk

def process_csv_to_csv(input_csv, output_csv, delimiter="|", chunk_size=1000, max_workers=2):
    """Reads a CSV file in chunks, fetches URL data concurrently, and writes to a new CSV file."""
    
    # Check if the output file exists to determine if we need to write headers
    file_exists = os.path.exists(output_csv)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        for chunk in pd.read_csv(input_csv, delimiter=delimiter, chunksize=chunk_size):
            # Process the chunk and fetch URL responses
            processed_chunk = process_chunk(chunk, executor)
            
            # Append the chunk to the output CSV (without writing header if the file exists)
            processed_chunk.to_csv(output_csv, mode='a', index=False, header=not file_exists)
            file_exists = True

# Usage
input_csv = "input.csv"  # Path to your input CSV
output_csv = "output.csv"  # Path to save the updated CSV
process_csv_to_csv(input_csv, output_csv)
