import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor

# Define file paths
input_file = '/mnt/data/test.csv'
output_file = '/mnt/data/updated_test.parquet'

# Function to make GET requests and return response text
def fetch_url(url):
    try:
        response = requests.get(url, timeout=10)
        return response.text
    except Exception as e:
        return str(e)

# Process the file in chunks
chunk_size = 100  # Adjust based on available memory
with pd.read_csv(input_file, delimiter='|', chunksize=chunk_size) as reader:
    for i, chunk in enumerate(reader):
        # Process URLs in the current chunk
        with ThreadPoolExecutor(max_workers=5) as executor:
            chunk['response_body'] = list(executor.map(fetch_url, chunk['urls']))

        # Append to the Parquet file
        chunk.to_parquet(
            output_file,
            engine='pyarrow',  # or 'fastparquet'
            index=False,
            compression='snappy',
            append=i > 0  # Append mode for subsequent chunks
        )

output_file
