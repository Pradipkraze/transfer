import pandas as pd
import requests
from concurrent.futures import ThreadPoolExecutor
import pyarrow.parquet as pq
import pyarrow as pa

# Function to fetch URL and return response
def fetch_url(url):
    try:
        response = requests.get(url, timeout=10)
        return response.text
    except requests.RequestException as e:
        return str(e)

# Function to process a chunk of data
def process_chunk(chunk, parquet_file):
    with ThreadPoolExecutor(max_workers=2) as executor:
        chunk['response_body'] = list(executor.map(fetch_url, chunk['urls']))

    # Append chunk to Parquet file
    table = pa.Table.from_pandas(chunk)
    with pq.ParquetWriter(parquet_file, table.schema, use_dictionary=True) as writer:
        writer.write_table(table)

# Main function
if __name__ == "__main__":
    input_file = "test.csv"
    parquet_file = "output.parquet"

    # Initialize Parquet file with schema
    first_chunk = pd.read_csv(input_file, sep='|', nrows=1)
    table = pa.Table.from_pandas(first_chunk)
    with pq.ParquetWriter(parquet_file, table.schema, use_dictionary=True) as writer:
        pass

    # Process CSV in chunks
    for chunk in pd.read_csv(input_file, sep='|', chunksize=100):
        process_chunk(chunk, parquet_file)
